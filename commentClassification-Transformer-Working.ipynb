{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9032/notebooks/Session_5_TorchText_practice/Untitled2.ipynb is exactly the same code with ofcourse the definition of encoer different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://www.manythings.org/anki/deu-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip deu-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in this cod we are reading teh data from the csv file ..this is important as most of the custom dataset will be in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.data import Field,TabularDataset,BucketIterator  \n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "import math\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('deu.txt', sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURLs(text):\n",
    "    URLless_string=text\n",
    "    print(text)\n",
    "    try:\n",
    "        \n",
    "        URLless_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "        print(\"removed url\")\n",
    "    except:\n",
    "        print(\"some error\")\n",
    "    return URLless_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df.columns=['English','German','Notes']\n",
    "# # subDf=df[['English','German']]\n",
    "\n",
    "# # subDf.to_csv(\"deu.csv\")\n",
    "# df = pd.read_csv('data/comments_all.csv')\n",
    "# df=df[['commentText','commentClassification']]\n",
    "# df['commentText']=df['commentText'].apply(lambda x:removeURLs(x))\n",
    "# df['commentClassification']=\"add \"+df['commentClassification']+\" end\"\n",
    "# df.to_csv(\"data/comments_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/comments_train.csv')\n",
    "# df\n",
    "\n",
    "#df=df[['commentText','commentClassification']]\n",
    "#df.to_csv(\"data/comments_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.comments_testAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields={'commentText':('SRC',SRC),'commentClassification':('TRG',TRG)}\n",
    "\n",
    "train_data,test_data=TabularDataset.splits(\n",
    "                                    path='data',\n",
    "                                    train='attentionTrain.csv',#this is the traing file\n",
    "                                    test='attentionTest.csv',##this is the test file\n",
    "                                    format='csv',\n",
    "                                    fields=fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 1)\n",
    "TRG.build_vocab(train_data, min_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['code', 'suggestion']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(test_data.examples[1])['TRG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRG.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,    5,   41,   88,   35,   11,   20,   51,   44,  167,    4,    3,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   14,  243,  362, 2197,  381,   47, 1077,   76,  801,    4,\n",
      "            3,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   22,   17,  546, 2428,    6,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   21,  162,  377,    4,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   49,   43,    4,  103,   99,    4,    3,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   22,  256, 1954,   13, 1061,    6,   11,   39,  784,   14,    3,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,  278,   35,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,  109,  178,  462,  585,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   18,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   51,   44,  167,    4,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    8,   79,   30,  491,   27,   83,  399,    9,   30,  323,  287,\n",
      "            7,  255,   74,   24, 1894,    4,    3,    1,    1],\n",
      "        [   2,   22,   31,   11,   62,  277,   74,   24,  184, 1448,   31,  117,\n",
      "         2244,    4,    6,    3,    1,    1,    1,    1,    1],\n",
      "        [   2,   18,  625,  211, 1101, 1101,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   35,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   34,    9,   34,   35,   30,   51,   44,   80,    3,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   18,  160,  146,  159,  152,  156,   22,   11,   31,  237,    8,\n",
      "           84,   95,    6,    3,    1,    1,    1,    1,    1],\n",
      "        [   2,   10,    8,  174,  125,   69,   19,  656,    6,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   38, 1253,  574,   19, 1457,  559,    4,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   34,    9,  187,   20,   27,  328,   88,  187,    4,    3,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   18,  655,  655,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   21,    8,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   22,   10,   96, 1063,  274,   29,    4,    6,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   55,   31,    7,   95,   29,    6,    3,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   75,    7,  140,   13,   56,    7,  341,  137,   12,  250,    4,\n",
      "            3,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    7,   33,   10, 1664,   20,   11,  273,    7,   81,   19,  115,\n",
      "          307,   13,  220,    9,    6,    3,    1,    1,    1],\n",
      "        [   2,  229,   12,   14,    7,  352,  296,  108,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5, 1971,    7,   61,   57,  760,  232,  733,   28,   25,    4,\n",
      "            3,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   21,  263,  128,    4,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   30, 1735,  255,   27, 1703,   24,   11,  178,   57,  245,  255,\n",
      "            4,   28,  554,   17,  340,  298,    8,    4,    3],\n",
      "        [   2,    9,   39,   27, 1147,  113,   28,   25,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   14,  134,   82,    5,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   21,  284,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "tensor([[ 2,  9,  4,  3],\n",
      "        [ 2, 11,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  9,  4,  3],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2,  8,  3,  1],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  9,  4,  3],\n",
      "        [ 2,  8,  3,  1],\n",
      "        [ 2,  8,  3,  1],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  9,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  8,  3,  1],\n",
      "        [ 2,  8,  3,  1],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2, 11,  4,  3],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2,  5,  4,  3]])\n",
      "i= 0\n",
      "[[2, 5, 41, 88, 35, 11, 20, 51, 44, 167, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 14, 243, 362, 2197, 381, 47, 1077, 76, 801, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1], [2, 22, 17, 546, 2428, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 21, 162, 377, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 49, 43, 4, 103, 99, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 22, 256, 1954, 13, 1061, 6, 11, 39, 784, 14, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 278, 35, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 109, 178, 462, 585, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 18, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 51, 44, 167, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 8, 79, 30, 491, 27, 83, 399, 9, 30, 323, 287, 7, 255, 74, 24, 1894, 4, 3, 1, 1], [2, 22, 31, 11, 62, 277, 74, 24, 184, 1448, 31, 117, 2244, 4, 6, 3, 1, 1, 1, 1, 1], [2, 18, 625, 211, 1101, 1101, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 35, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 34, 9, 34, 35, 30, 51, 44, 80, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 18, 160, 146, 159, 152, 156, 22, 11, 31, 237, 8, 84, 95, 6, 3, 1, 1, 1, 1, 1], [2, 10, 8, 174, 125, 69, 19, 656, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 38, 1253, 574, 19, 1457, 559, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 34, 9, 187, 20, 27, 328, 88, 187, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 18, 655, 655, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 21, 8, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 22, 10, 96, 1063, 274, 29, 4, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 55, 31, 7, 95, 29, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 75, 7, 140, 13, 56, 7, 341, 137, 12, 250, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1], [2, 7, 33, 10, 1664, 20, 11, 273, 7, 81, 19, 115, 307, 13, 220, 9, 6, 3, 1, 1, 1], [2, 229, 12, 14, 7, 352, 296, 108, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 1971, 7, 61, 57, 760, 232, 733, 28, 25, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1], [2, 21, 263, 128, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 30, 1735, 255, 27, 1703, 24, 11, 178, 57, 245, 255, 4, 28, 554, 17, 340, 298, 8, 4, 3], [2, 9, 39, 27, 1147, 113, 28, 25, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 14, 134, 82, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 21, 284, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "update\n",
      "once\n",
      "verified\n",
      "we\n",
      "can\n",
      "merge\n",
      "after\n",
      "verification\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "use\n",
      "proper\n",
      "naming\n",
      "res\n",
      "doesn\n",
      "t\n",
      "imply\n",
      "any\n",
      "meaning\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "why\n",
      "not\n",
      "generic\n",
      "uploadresponse\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "remove\n",
      "log\n",
      "command\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "correct\n",
      "indentation\n",
      ".\n",
      "single\n",
      "tabs\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "why\n",
      "both\n",
      "mapperutils\n",
      "and\n",
      "gson\n",
      "?\n",
      "we\n",
      "should\n",
      "ideally\n",
      "use\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "qa\n",
      "verified\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "just\n",
      "return\n",
      "collections\n",
      ".singleton\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      " \n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "merge\n",
      "after\n",
      "verification\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "this\n",
      "condition\n",
      "will\n",
      "never\n",
      "be\n",
      "used\n",
      "since\n",
      "it\n",
      "will\n",
      "go\n",
      "inside\n",
      "the\n",
      "block\n",
      "only\n",
      "if\n",
      "isserialnumbersetting\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "why\n",
      "are\n",
      "we\n",
      "using\n",
      "first\n",
      "only\n",
      "if\n",
      "multiple\n",
      "beatids\n",
      "are\n",
      "being\n",
      "searched\n",
      ".\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      " \n",
      "export\n",
      "const\n",
      "ledgers\n",
      "ledgers\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "verified\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "get\n",
      "it\n",
      "get\n",
      "verified\n",
      "will\n",
      "merge\n",
      "after\n",
      "that\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      " \n",
      "ffc\n",
      "ed\n",
      "dbf\n",
      "da\n",
      "ae\n",
      "why\n",
      "we\n",
      "are\n",
      "getting\n",
      "this\n",
      "file\n",
      "changes\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "is\n",
      "this\n",
      "method\n",
      "already\n",
      "there\n",
      "in\n",
      "parent\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "add\n",
      "today\n",
      "support\n",
      "in\n",
      "bind\n",
      "parameter\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "get\n",
      "it\n",
      "tested\n",
      "can\n",
      "be\n",
      "merged\n",
      "once\n",
      "tested\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      " \n",
      "paper\n",
      "paper\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "remove\n",
      "this\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "why\n",
      "is\n",
      "default\n",
      "gstinvoicetemplate\n",
      "changed\n",
      "here\n",
      ".\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "what\n",
      "are\n",
      "the\n",
      "changes\n",
      "here\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "resolve\n",
      "the\n",
      "conflicts\n",
      "and\n",
      "change\n",
      "the\n",
      "target\n",
      "branch\n",
      "to\n",
      "development\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "the\n",
      "code\n",
      "is\n",
      "duplicated\n",
      "can\n",
      "we\n",
      "write\n",
      "the\n",
      "function\n",
      "in\n",
      "one\n",
      "model\n",
      "and\n",
      "call\n",
      "it\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "better\n",
      "to\n",
      "use\n",
      "the\n",
      "custom\n",
      "widget\n",
      "constants\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "mege\n",
      "the\n",
      "query\n",
      "from\n",
      "drop\n",
      "column\n",
      "batch\n",
      "i\n",
      "d\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "remove\n",
      "invalid\n",
      "spaces\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "will\n",
      "finally\n",
      "block\n",
      "be\n",
      "executed\n",
      "if\n",
      "we\n",
      "return\n",
      "from\n",
      "try\n",
      "block\n",
      ".\n",
      "i\n",
      "m\n",
      "not\n",
      "sure\n",
      "about\n",
      "this\n",
      ".\n",
      "<eos>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "it\n",
      "should\n",
      "be\n",
      "orderids\n",
      "array\n",
      "i\n",
      "d\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "use\n",
      "constant\n",
      "variable\n",
      "please\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "remove\n",
      "comments\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "tensor([[   2,   48,   12,   54,   15,   66,   79,   16,  256,   28,   25,   13,\n",
      "          620,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,  622,   11,  477,  258,    4,    5, 1686,    7,   66,    3,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,    8, 2301,  308,  106,   15,  377,    4,   27, 1781,   12,    7,\n",
      "          112,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,  207,   16,    8,    6,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   18,    4,    4,    4,  204,   97, 1903,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,  348,    5,    4,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,    5,   34,    9,   35,   13,   41,   50,   11,   30,   51,   88,\n",
      "           35,    4,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,  245,   12, 1243,    7,   79,    9,   30,   27,  229,    4,    3,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   61,    5,    4,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,    5,   72,   47,  161,   33,  107,   19,    7,   33,    4,   34,\n",
      "            9,   57,  107,  150,    4,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   21,   70,   33,   21,  124,  102,    3,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   49,   43,    4,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,    5,   34,    9,   35,   13,   41,   50,   16,   51,    4,    3,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   21,    8,   56,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   53,   44,    5,   14,   53,   44,   13,   89,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   24,   11,  676,  176,   28,   25,   77,  239,  176,    4,   24,\n",
      "          239,  155,   77,   14,  155,   28,   25,   19, 2052,    4,    3,    1,\n",
      "            1],\n",
      "        [   2,   22,   31,   37,  283,  282,   29,  192,    7,   66,  579,   10,\n",
      "          248,   19, 1783,    6,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2, 2107,   75,    7,  140,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,    5,  333,    7,   50,   19,  250,  137,    4,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   14,   16,  856,   19,  200,   16,  229,  498,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   53,   44,  221,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,  147,  300,  708, 2100,   39,   27,  633,   57,    7,  954,    4,\n",
      "            3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   82,  362,    4,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   21,   70,   67,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   36,   60,  758,   88,  295,   11,   48,    7,  909,    3,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   24,   11,   72,   47,   48,    8,   70,   33,    5,   21,    9,\n",
      "            4,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   61,   10,   52,    4,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   93,   24,  210,   20,   27, 1523,   13,  141,  369,   19,  115,\n",
      "          205,    6,    5,   46,    7,  809,   95,    4,    3,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   17,   52,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   18, 1793,   20,   27,  173,  126,   98,  251,   71,   23,  321,\n",
      "          251,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   20,   11,  337,    7, 1780,   54,  711,  147,   83,   63,   17,\n",
      "            4,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1],\n",
      "        [   2,   11,   48,   12,  347,    8,  280,   44,  283,   60,  127,  342,\n",
      "            4,   16,  135,  225,    9,  323,   32,    9,   10, 2432,   87,    4,\n",
      "            3]])\n",
      "tensor([[ 2,  7,  4,  3],\n",
      "        [ 2, 11,  4,  3],\n",
      "        [ 2, 12,  4,  3],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  9,  4,  3],\n",
      "        [ 2, 12,  4,  3],\n",
      "        [ 2,  9,  4,  3],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  9,  4,  3],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2, 12,  4,  3],\n",
      "        [ 2,  8,  3,  1],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2, 12,  4,  3],\n",
      "        [ 2, 11,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2, 12,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  9,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2, 12,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  5,  4,  3]])\n",
      "i= 1\n",
      "[[2, 48, 12, 54, 15, 66, 79, 16, 256, 28, 25, 13, 620, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 622, 11, 477, 258, 4, 5, 1686, 7, 66, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 8, 2301, 308, 106, 15, 377, 4, 27, 1781, 12, 7, 112, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 207, 16, 8, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 18, 4, 4, 4, 204, 97, 1903, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 348, 5, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 34, 9, 35, 13, 41, 50, 11, 30, 51, 88, 35, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 245, 12, 1243, 7, 79, 9, 30, 27, 229, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 61, 5, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 72, 47, 161, 33, 107, 19, 7, 33, 4, 34, 9, 57, 107, 150, 4, 3, 1, 1, 1, 1, 1, 1, 1], [2, 21, 70, 33, 21, 124, 102, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 49, 43, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 34, 9, 35, 13, 41, 50, 16, 51, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 21, 8, 56, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 53, 44, 5, 14, 53, 44, 13, 89, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 24, 11, 676, 176, 28, 25, 77, 239, 176, 4, 24, 239, 155, 77, 14, 155, 28, 25, 19, 2052, 4, 3, 1, 1], [2, 22, 31, 37, 283, 282, 29, 192, 7, 66, 579, 10, 248, 19, 1783, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2107, 75, 7, 140, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 333, 7, 50, 19, 250, 137, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 14, 16, 856, 19, 200, 16, 229, 498, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 53, 44, 221, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 147, 300, 708, 2100, 39, 27, 633, 57, 7, 954, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 82, 362, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 21, 70, 67, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 36, 60, 758, 88, 295, 11, 48, 7, 909, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 24, 11, 72, 47, 48, 8, 70, 33, 5, 21, 9, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 61, 10, 52, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 93, 24, 210, 20, 27, 1523, 13, 141, 369, 19, 115, 205, 6, 5, 46, 7, 809, 95, 4, 3, 1, 1, 1, 1], [2, 17, 52, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 18, 1793, 20, 27, 173, 126, 98, 251, 71, 23, 321, 251, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 20, 11, 337, 7, 1780, 54, 711, 147, 83, 63, 17, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 11, 48, 12, 347, 8, 280, 44, 283, 60, 127, 342, 4, 16, 135, 225, 9, 323, 32, 9, 10, 2432, 87, 4, 3]]\n",
      "length of file 25\n",
      "<sos>\n",
      "need\n",
      "to\n",
      "have\n",
      "a\n",
      "same\n",
      "condition\n",
      "for\n",
      "both\n",
      "i\n",
      "d\n",
      "and\n",
      "erpid\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "everywhere\n",
      "we\n",
      "follow\n",
      "camelcase\n",
      ".\n",
      "please\n",
      "ensure\n",
      "the\n",
      "same\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "this\n",
      "sounds\n",
      "more\n",
      "like\n",
      "a\n",
      "command\n",
      ".\n",
      "be\n",
      "gentle\n",
      "to\n",
      "the\n",
      "user\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "ticket\n",
      "for\n",
      "this\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      " \n",
      ".\n",
      ".\n",
      ".\n",
      "new\n",
      "set\n",
      "joinedtables\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "cleanup\n",
      "please\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "please\n",
      "get\n",
      "it\n",
      "verified\n",
      "and\n",
      "update\n",
      "pr\n",
      "we\n",
      "will\n",
      "merge\n",
      "once\n",
      "verified\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "try\n",
      "to\n",
      "swap\n",
      "the\n",
      "condition\n",
      "it\n",
      "will\n",
      "be\n",
      "better\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "query\n",
      "please\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "please\n",
      "don\n",
      "t\n",
      "hard\n",
      "code\n",
      "strings\n",
      "in\n",
      "the\n",
      "code\n",
      ".\n",
      "get\n",
      "it\n",
      "from\n",
      "strings\n",
      ".xml\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "remove\n",
      "commented\n",
      "code\n",
      "remove\n",
      "unnecessary\n",
      "lines\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "correct\n",
      "indentation\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "please\n",
      "get\n",
      "it\n",
      "verified\n",
      "and\n",
      "update\n",
      "pr\n",
      "for\n",
      "merge\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "remove\n",
      "this\n",
      "change\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "space\n",
      "after\n",
      "please\n",
      "use\n",
      "space\n",
      "after\n",
      "and\n",
      "before\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "if\n",
      "we\n",
      "saving\n",
      "outlet\n",
      "i\n",
      "d\n",
      "then\n",
      "its\n",
      "outlet\n",
      ".\n",
      "if\n",
      "its\n",
      "warehouse\n",
      "then\n",
      "use\n",
      "warehouse\n",
      "i\n",
      "d\n",
      "in\n",
      "orgid\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "why\n",
      "are\n",
      "you\n",
      "checking\n",
      "again\n",
      "here\n",
      "when\n",
      "the\n",
      "same\n",
      "thing\n",
      "is\n",
      "checked\n",
      "in\n",
      "getactionfetcher\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "plesae\n",
      "resolve\n",
      "the\n",
      "conflicts\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "please\n",
      "raise\n",
      "the\n",
      "pr\n",
      "in\n",
      "development\n",
      "branch\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "use\n",
      "for\n",
      "statement\n",
      "in\n",
      "else\n",
      "for\n",
      "better\n",
      "readability\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "space\n",
      "after\n",
      "comma\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "index\n",
      "based\n",
      "vendor\n",
      "pickup\n",
      "should\n",
      "be\n",
      "hidden\n",
      "from\n",
      "the\n",
      "caller\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "variable\n",
      "naming\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "remove\n",
      "commented\n",
      "line\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "check\n",
      "with\n",
      "design\n",
      "once\n",
      "whether\n",
      "we\n",
      "need\n",
      "the\n",
      "accordion\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "if\n",
      "we\n",
      "don\n",
      "t\n",
      "need\n",
      "this\n",
      "commented\n",
      "code\n",
      "please\n",
      "remove\n",
      "it\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "query\n",
      "is\n",
      "required\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "these\n",
      "if\n",
      "conditions\n",
      "can\n",
      "be\n",
      "combined\n",
      "and\n",
      "put\n",
      "them\n",
      "in\n",
      "one\n",
      "right\n",
      "?\n",
      "please\n",
      "do\n",
      "the\n",
      "necessary\n",
      "changes\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "not\n",
      "required\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      " \n",
      "getter\n",
      "can\n",
      "be\n",
      "done\n",
      "at\n",
      "class\n",
      "level\n",
      "instead\n",
      "of\n",
      "field\n",
      "level\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "can\n",
      "we\n",
      "show\n",
      "the\n",
      "genericformdatanew\n",
      "have\n",
      "zero\n",
      "index\n",
      "used\n",
      "or\n",
      "not\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 25\n",
      "<sos>\n",
      "we\n",
      "need\n",
      "to\n",
      "clean\n",
      "this\n",
      "up\n",
      "after\n",
      "checking\n",
      "with\n",
      "other\n",
      "team\n",
      ".\n",
      "for\n",
      "now\n",
      "let\n",
      "it\n",
      "go\n",
      "as\n",
      "it\n",
      "is\n",
      "urgent\n",
      "fix\n",
      ".\n",
      "<eos>\n",
      "------\n",
      "tensor([[   2,   22,   10,    8,   52,    6,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   21,   70,  695,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   75,  140,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   79,   16,  878,   58,    6,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   21,   44,  377,    4,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   10,    9,   48,   16,  642,    6,    6,    6,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   28,  431,   37,   54,  987,    7,  288,   59,   65,    4,    5,\n",
      "          404,    9,  589,   80,    3,    1,    1,    1,    1],\n",
      "        [   2,   20,   27,   19,  108,   84,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   18,   26,   15,   26,   40,   15,   42,   42,    9,   39,   27,\n",
      "          169,  895,   36,    3,    1,    1,    1,    1,    1],\n",
      "        [   2,   38,   49,  128, 1882,  276,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   75,    7,  140,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   22,   10,    8,  110,    6,   13,   31,   11,   62,    9,   19,\n",
      "           76,   23,  652,  566,    6,    3,    1,    1,    1],\n",
      "        [   2,   34,    7,   90,  387,   57,  107,  150,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   97,  341,   12,  250,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   21,  129,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   87,   43,    4,   14,   99,    4,    3,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   34,    8,   35,    4,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   24,    8,   82,  327, 1983,    5,  122,    4,    3,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   38,  445,  218,   36,   13,   65,   75,  140,    3,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   38,   36,   16,  441,   13,  570,  875,  505,    3,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   53,   89,   13,   44,    4,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   20,   11,  658,    9,   57,  195,    6,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   75,    7,  105,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   21,   70,   33,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,  292,   61,   12,   36,    7,   14,   23,  147,  200,   11,\n",
      "           30,   54,   12,  855,    7,   63,   79,    4,    3],\n",
      "        [   2,   22,  110,  118,   36,    6,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,   14,  257,   16,  103,   67,   24,  210,   65,    4,    3,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2, 2286,  441,  637,   10,  117,   83,    4,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,   61,   10,   52,   29,   65,    4,    3,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,  122,    8,    6,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,  997,   65,   20,   11,  121,    8,   19,   15,  134,   84,    6,\n",
      "            3,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,    5,  104,   50,   16,  379,  137,    4,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "tensor([[ 2,  8,  3,  1],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2, 12,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  8,  3,  1],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  9,  4,  3],\n",
      "        [ 2, 11,  4,  3],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  8,  3,  1],\n",
      "        [ 2, 10,  4,  3],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2,  8,  3,  1],\n",
      "        [ 2,  5,  4,  3],\n",
      "        [ 2,  7,  4,  3],\n",
      "        [ 2,  9,  4,  3],\n",
      "        [ 2, 11,  4,  3],\n",
      "        [ 2,  6,  4,  3],\n",
      "        [ 2, 10,  4,  3]])\n",
      "i= 2\n",
      "[[2, 22, 10, 8, 52, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 21, 70, 695, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 75, 140, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 79, 16, 878, 58, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 21, 44, 377, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 10, 9, 48, 16, 642, 6, 6, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 28, 431, 37, 54, 987, 7, 288, 59, 65, 4, 5, 404, 9, 589, 80, 3, 1, 1, 1, 1], [2, 20, 27, 19, 108, 84, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 18, 26, 15, 26, 40, 15, 42, 42, 9, 39, 27, 169, 895, 36, 3, 1, 1, 1, 1, 1], [2, 38, 49, 128, 1882, 276, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 75, 7, 140, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 22, 10, 8, 110, 6, 13, 31, 11, 62, 9, 19, 76, 23, 652, 566, 6, 3, 1, 1, 1], [2, 34, 7, 90, 387, 57, 107, 150, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 97, 341, 12, 250, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 21, 129, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 87, 43, 4, 14, 99, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 34, 8, 35, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 24, 8, 82, 327, 1983, 5, 122, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 38, 445, 218, 36, 13, 65, 75, 140, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 38, 36, 16, 441, 13, 570, 875, 505, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 53, 89, 13, 44, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 20, 11, 658, 9, 57, 195, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 75, 7, 105, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 21, 70, 33, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 292, 61, 12, 36, 7, 14, 23, 147, 200, 11, 30, 54, 12, 855, 7, 63, 79, 4, 3], [2, 22, 110, 118, 36, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 14, 257, 16, 103, 67, 24, 210, 65, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2286, 441, 637, 10, 117, 83, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 61, 10, 52, 29, 65, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 122, 8, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 997, 65, 20, 11, 121, 8, 19, 15, 134, 84, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1], [2, 5, 104, 50, 16, 379, 137, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "length of file 21\n",
      "<sos>\n",
      "why\n",
      "is\n",
      "this\n",
      "required\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "remove\n",
      "commented\n",
      "styling\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "resolve\n",
      "conflicts\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "condition\n",
      "for\n",
      "warehouseid\n",
      "!\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "remove\n",
      "after\n",
      "command\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "is\n",
      "it\n",
      "need\n",
      "for\n",
      "logging\n",
      "?\n",
      "?\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "i\n",
      "hope\n",
      "you\n",
      "have\n",
      "considered\n",
      "the\n",
      "map\n",
      "data\n",
      "also\n",
      ".\n",
      "please\n",
      "verify\n",
      "it\n",
      "against\n",
      "that\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "can\n",
      "be\n",
      "in\n",
      "constants\n",
      "file\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      " \n",
      "c\n",
      "a\n",
      "c\n",
      "b\n",
      "a\n",
      "e\n",
      "e\n",
      "it\n",
      "should\n",
      "be\n",
      "product\n",
      ".please\n",
      "check\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "add\n",
      "correct\n",
      "spaces\n",
      "isapi\n",
      "false\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "resolve\n",
      "the\n",
      "conflicts\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "why\n",
      "is\n",
      "this\n",
      "removed\n",
      "?\n",
      "and\n",
      "are\n",
      "we\n",
      "using\n",
      "it\n",
      "in\n",
      "any\n",
      "of\n",
      "our\n",
      "reports\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "get\n",
      "the\n",
      "string\n",
      "literal\n",
      "from\n",
      "strings\n",
      ".xml\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "set\n",
      "target\n",
      "to\n",
      "development\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "remove\n",
      "comment\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "fix\n",
      "indentation\n",
      ".\n",
      "use\n",
      "tabs\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "get\n",
      "this\n",
      "verified\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "if\n",
      "this\n",
      "variable\n",
      "means\n",
      "modifiedoldlayouts\n",
      "please\n",
      "rename\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "add\n",
      "server\n",
      "version\n",
      "check\n",
      "and\n",
      "also\n",
      "resolve\n",
      "conflicts\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "add\n",
      "check\n",
      "for\n",
      "primary\n",
      "and\n",
      "secondary\n",
      "van\n",
      "sale\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "space\n",
      "before\n",
      "and\n",
      "after\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "can\n",
      "we\n",
      "pick\n",
      "it\n",
      "from\n",
      "theme\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "resolve\n",
      "the\n",
      "conflict\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "remove\n",
      "commented\n",
      "code\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "share\n",
      "query\n",
      "to\n",
      "check\n",
      "the\n",
      "use\n",
      "of\n",
      "index\n",
      "else\n",
      "we\n",
      "will\n",
      "have\n",
      "to\n",
      "split\n",
      "the\n",
      "or\n",
      "condition\n",
      ".\n",
      "<eos>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "why\n",
      "removed\n",
      "null\n",
      "check\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "use\n",
      "brackets\n",
      "for\n",
      "single\n",
      "line\n",
      "if\n",
      "conditions\n",
      "also\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "skucodeindex\n",
      "primary\n",
      "indexes\n",
      "is\n",
      "being\n",
      "used\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "query\n",
      "is\n",
      "required\n",
      "here\n",
      "also\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "rename\n",
      "this\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "currencysign\n",
      "also\n",
      "can\n",
      "we\n",
      "keep\n",
      "this\n",
      "in\n",
      "a\n",
      "constant\n",
      "file\n",
      "?\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n",
      "length of file 21\n",
      "<sos>\n",
      "please\n",
      "create\n",
      "pr\n",
      "for\n",
      "dev\n",
      "branch\n",
      ".\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(train_iterator):\n",
    "    print(batch.SRC)\n",
    "    print(batch.TRG)\n",
    "    print(\"i=\",i)\n",
    "    lst=batch.SRC.tolist()\n",
    "    print(lst)\n",
    "    for b in lst:\n",
    "        print(\"length of file\", len(b))\n",
    "        for j in b:\n",
    "            print(SRC.vocab.itos[j])\n",
    "        print(\"------\")\n",
    "    #print(vars(train_data.examples[i])['TRG'])\n",
    "    if(i==2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['check',\n",
       " 'for',\n",
       " 'null',\n",
       " 'domain',\n",
       " 'url',\n",
       " 'add',\n",
       " 'log',\n",
       " 'also',\n",
       " 'if',\n",
       " 'url',\n",
       " 'is',\n",
       " 'null']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data.examples[1])['SRC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we will start building the encoder class\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,#length of the vocabulary\n",
    "                 hid_dim,#512\n",
    "                 n_layers,#6\n",
    "                 n_heads,#8\n",
    "                 pf_dim,#output dimention\n",
    "                 dropout,\n",
    "                 device,#cuda\n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        #input embedding\n",
    "        self.tok_embedding=nn.Embedding(input_dim,hid_dim)\n",
    "        ## convert the indices of the words to the position embedding. since there can be just \n",
    "        ## 100 tokens in the sentence hence the embedding will be a matrix of 100*hid_dim\n",
    "        self.pos_embedding=nn.Embedding(max_length,hid_dim) \n",
    "        \n",
    "        ##create layers ..what happens in each layer is listed\n",
    "        self.layers=nn.ModuleList([EncoderLayer(hid_dim, ## this is 512\n",
    "                                                n_heads, ## MHA has n_heads\n",
    "                                                pf_dim, ## the dimentions of output\n",
    "                                                dropout,\n",
    "                                                device\n",
    "                                                )for _ in range(n_layers)])\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##define dropout\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.scale=torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    \n",
    "    ##src=it is a set of input values that are in the form of batch\n",
    "    ##after the Encoder running will get the inputs for the decoder\n",
    "    def forward(self,src,src_mask):\n",
    "        batch_size=src.shape[0]\n",
    "        src_len=src.shape[1]\n",
    "        \n",
    "        ##it will fill the pos vector with the src_len numbers which are repeated batch_size\n",
    "        ## also for the combination to happen between the pos vector and the input they \n",
    "        ## must be of the same dimention\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        ##this is how the batch will look like after combining the scaling factor, positional embedding\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        \n",
    "        ##pass it to all the layers...these layers is the MHA & feedforward network + residual connections\n",
    "        ##after the operations also the same variable src is being updated\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        return src\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the two norm layers\n",
    "        self.self_attn_layer_norm=nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm=nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        ## MultiHeadAttentionLayer is a class that we will call\n",
    "        self.self_attention=MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        ## the feedforward\n",
    "        ## PositionwiseFeedforwardLayer is a class that will be called\n",
    "        self.positionwise_feedforward=PositionwiseFeedforwardLayer(hid_dim,pf_dim,dropout)\n",
    "        \n",
    "    ##src is the batch wise data\n",
    "    def forward(self,src,src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]...basically each token will be of hid_dim\n",
    "        ##[[.1,.1,.2,.3,.4,.4,...512 entries]\n",
    "        ##[.1,.1,.2,.3,.4,.4,...512 entries]] two tokens whose embedding is 512 length and are in the form of a batch\n",
    "        \n",
    "        \n",
    "        #self_attention...it will take the query,key,value matrix and then calculate the self attention. Also to use\n",
    "        ## the same code for the decoder by passing a src_mask\n",
    "        #_src,_=self.self_attention(query=src,key=src,value=src,mask=src_mask)\n",
    "        \n",
    "        _src,_=self.self_attention(src,src,src,src_mask)\n",
    "        \n",
    "        ##apply the dropout, residual and pass to norm\n",
    "        src=self.self_attn_layer_norm(src+self.dropout(_src))\n",
    "        \n",
    "        \n",
    "        _src=self.positionwise_feedforward(src)\n",
    "        \n",
    "        src=self.ff_layer_norm(src+self.dropout(_src))\n",
    "        \n",
    "        return src\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "    \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim=hid_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.head_dim=hid_dim // n_heads\n",
    "        \n",
    "        ##this is as good as definng a Wq...when ever we will have a linear layer there will be a W associated\n",
    "        ## also instead of decalring it for the head_dim we are declaring it for the hidden dimention and then \n",
    "        ## we will divide the matrix into the n_heads.. this way the code is much general\n",
    "        self.fc_q=nn.Linear(hid_dim,hid_dim)\n",
    "        self.fc_k=nn.Linear(hid_dim,hid_dim)\n",
    "        self.fc_v=nn.Linear(hid_dim,hid_dim)\n",
    "        \n",
    "        ## this is the last weight matrix that will be used at the time of concatenation of the outputs        \n",
    "        self.fc_o=nn.Linear(hid_dim,hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        ##the scaling factor as in scale dot products...this will be the sqrt of the dimention that we are using\n",
    "        self.scale=torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    \n",
    "    \n",
    "    ## there are lot of transformation happeing\n",
    "    ## [batch,word,hid_dim] -> [batch,word,n_head,head_dim] (view)-> [batch,n_head,word,head_dim] (permute)\n",
    "    ## ->[batch,words,number_heads,head_dim] (permute +contiguous) -> [batch,words,hid_dim] (view)\n",
    "    ##\n",
    "    ##\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        \n",
    "        #assuming that the matrices have an additional batch dimention\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #Q=[batch,word,hid_dim]\n",
    "        Q=self.fc_q(query)\n",
    "        K=self.fc_k(key)\n",
    "        V=self.fc_v(value)\n",
    "        \n",
    "        #Q is of the dimention [batch,word,hid_dim]\n",
    "        # we want to create new Q which is [batch,num_head,words,head_dim]\n",
    "        # it is a 2 step thing...\n",
    "        # step1 [batch,word,hid_dim] -> [batch,word,n_head,head_dim] using views\n",
    "        #step2 using permute change the axis from [batch,word,n_head,head_dim] -> [batch,n_head,word,head_dim]\n",
    "        Q=Q.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        K=K.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        V=V.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        \n",
    "        ## now compuet the energy ..we need to take a transpose of K\n",
    "        ## this will give us energy dimentions as [batch_len,number_heads,number_words,number_words]\n",
    "        energy=torch.matmul(Q,K.permute(0,1,3,2))/self.scale\n",
    "        #0, 1, 3, 2\n",
    "        ## this is how we will use the mask\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        #attentiox=[batch,number_heads,words,words]\n",
    "        attention=torch.softmax(energy,dim=-1)\n",
    "        \n",
    "        ##this will give x=[batch,number_heads,words,head_dim]\n",
    "        x=torch.matmul(self.dropout(attention),V)\n",
    "        \n",
    "        #batchnumber,words,heads,head_dim..this is some memory optimization operation\n",
    "        # but the x is also changed from [batch,number_heads,words,head_dim] -> [batch,words,number_heads,head_dim]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #now number_heads and head_dim are getting combined together to get the single continuous tensor\n",
    "        #[batch,words,number_heads,head_dim] -> [batch,words,hid_dim]\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        \n",
    "        ##pass it through the linear layer\n",
    "        x=self.fc_o(x)\n",
    "        \n",
    "        \n",
    "        #x=x = [batch size, words, hid dim]\n",
    "        return x,attention\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##positionwise feed forward layer\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x = [batch size, seq len, hid_dim]->[batch size, seq len, pf_dim]    \n",
    "        x=self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x=[batch size, seq len, pf_dim]->[batch size, seq len, hid_dim]\n",
    "        x=self.fc_2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##decoder\n",
    "##this should be simialr to the encoder class so I am strating by copying the encoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim,#length of the vocabulary\n",
    "                 hid_dim,#512\n",
    "                 n_layers,#6\n",
    "                 n_heads,#8\n",
    "                 pf_dim,#output dimention\n",
    "                 dropout,\n",
    "                 device,#cuda\n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        #input embedding\n",
    "        self.tok_embedding=nn.Embedding(output_dim,hid_dim)\n",
    "        ## convert the indices of the words to the position embedding. since there can be just \n",
    "        ## 100 tokens in the sentence hence the embedding will be a matrix of 100*hid_dim\n",
    "        self.pos_embedding=nn.Embedding(max_length,hid_dim) \n",
    "        \n",
    "        ##create layers ..what happens in each layer is listed\n",
    "        self.layers=nn.ModuleList([DecoderLayer(hid_dim, ## this is 512\n",
    "                                                n_heads, ## MHA has n_heads\n",
    "                                                pf_dim, ## the dimentions of output\n",
    "                                                dropout,\n",
    "                                                device\n",
    "                                                )\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        \n",
    "        ##thiis is extra\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        ##define dropout\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.scale=torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    \n",
    "    ## the forward function will be little different than the Encoder\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        batch_size=trg.shape[0]\n",
    "        trg_len=trg.shape[1]\n",
    "        \n",
    "        ##it will fill the pos vector with the src_len numbers which are repeated batch_size\n",
    "        ## also for the combination to happen between the pos vector and the input they \n",
    "        ## must be of the same dimention\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        ##this is how the batch will look like after combining the scaling factor, positional embedding\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        \n",
    "        ##pass it to all the layers...these layers is the MHA & feedforward network + residual connections\n",
    "        ##after the operations also the same variable src is being updated\n",
    "        for layer in self.layers:\n",
    "            trg,attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        output=self.fc_out(trg)\n",
    "        return output,attention\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        #norm for the self attention\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        #norm for the encoder _attentions\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        #the feedfoward layer normalization..this one is after the encoder-attentions\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        ## calculate the self attentions\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        \n",
    "        # calculate the attention using the K,V coming from encoder\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        \n",
    "        #feedforward\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,trg,enc_src,trg_mask,src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "            \n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #encoder attention Q is coming from Decoder while K and V are coming from encoder\n",
    "        # the src_mask is to stop the decoder from using the <PAD> values in case if any\n",
    "        #enc_src=[batchsize,source_len,hid_dim]\n",
    "        #src_mask=[batbatch_size,source_len]\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        _trg=self.positionwise_feedforward(trg)\n",
    "        \n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        return trg,attention\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this module will encapsulate the encoder -decoder piece . it will also take care of the maskings\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    #this is just to add the 0s at the places which are pad\n",
    "    def make_src_mask(self, src):        \n",
    "        #src = [batch size, src len] \n",
    "        ## we are chaecking if the word is not a <PAD>..\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        #first check for the padding and make them 0\n",
    "        ##this is also checking if the word is not a PAD\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "\n",
    "        #if the padding is giving some more locatiosn as 0 then include them as well\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        return trg_mask\n",
    "         \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "    def forward(self,src,trg):\n",
    "            \n",
    "        #src_mask=[batch size,1,1,src len]\n",
    "        src_mask = self.make_src_mask(src)\n",
    "\n",
    "        #trg_mask=[batch size,1,1,trg len]\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        #enc_src=[batch size, src len, hid dim]\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        ##output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n_heads, trg len, src len]  ..in case of the encoder it is bs,nhead,src_len,src_len\n",
    "        ## but in the case of the decoder Q is coming from decoder(aka target) so trg_len and \n",
    "        ## K is coming from encoder so src_len\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        return output,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,652,303 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##weight initialization\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## this is the code to make sure that all the sentences are propelry formed that is required for the trainign\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # helper function to club together sequential operations\n",
    "# def sequential_transforms(*transforms):\n",
    "#     def func(txt_input):\n",
    "#         for transform in transforms:\n",
    "#             txt_input = transform(txt_input)\n",
    "#         return txt_input\n",
    "#     return func\n",
    "\n",
    "# # function to add BOS/EOS and create tensor for input sequence indices\n",
    "# def tensor_transform(token_ids: List[int]):\n",
    "#     return torch.cat((torch.tensor([BOS_IDX]), \n",
    "#                       torch.tensor(token_ids), \n",
    "#                       torch.tensor([EOS_IDX])))\n",
    "\n",
    "# # src and tgt language text transforms to convert raw strings into tensors indices\n",
    "# text_transform = {}\n",
    "# for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "#     text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "#                                                vocab_transform[ln], #Numericalization\n",
    "#                                                tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# # function to collate data samples into batch tesors\n",
    "# def collate_fn(batch):\n",
    "#     src_batch, tgt_batch = [], []\n",
    "#     for src_sample, tgt_sample in batch:\n",
    "#         src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "#         tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "#     src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "#     tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "#     return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# def train_epoch(model, optimizer):\n",
    "#     model.train()\n",
    "#     losses = 0\n",
    "# #     train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "# #     train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "#     #for src, tgt in train_dataloader:\n",
    "#     count=0\n",
    "#     for i,batch in enumerate(train_iterator):\n",
    "#         src = batch.SRC.to(device)\n",
    "#         tgt = batch.TRG.to(device)\n",
    "#         # print(src.shape)\n",
    "#         # print(tgt.shape)\n",
    "#         # print(tgt[:,:-1].shape)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         ##some issue here...need to check\n",
    "#         output, _ = model(src, tgt[:,:-1])\n",
    "\n",
    "#         output_dim = output.shape[-1]\n",
    "            \n",
    "#         output = output.contiguous().view(-1, output_dim)\n",
    "#         tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "#         # output = output[1:].view(-1, output.shape[-1])\n",
    "#         # tgt = tgt[1:].reshape(-1) #tgt[:,:-1][1:].reshape(-1)\n",
    "#         loss = loss_fn(output, tgt)\n",
    "#         loss.backward()\n",
    "#         clip = 1\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "#         optimizer.step()\n",
    "#         losses += loss.item()\n",
    "#         count+=1\n",
    "\n",
    "#     return losses / count\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "#     train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "#     train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    #for src, tgt in train_dataloader:\n",
    "    count=0\n",
    "    for i,batch in enumerate(train_iterator):\n",
    "        try:\n",
    "            src = batch.SRC.to(device)\n",
    "            tgt = batch.TRG.to(device)\n",
    "            # print(src.shape)\n",
    "#             print(tgt.shape)\n",
    "#             print(tgt[:,:-1].shape)\n",
    "#             print(i)\n",
    "\n",
    "#             print(batch.SRC)\n",
    "#             print(batch.TRG)\n",
    "#             print(\"i=\",i)\n",
    "#             lst=batch.SRC.tolist()\n",
    "#             print(lst)\n",
    "#             for b in lst:\n",
    "#                 print(\"length of file\", len(b))\n",
    "#                 for j in b:\n",
    "#                     print(SRC.vocab.itos[j])\n",
    "#                 print(\"------\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(src, tgt[:,:-1]) #[:,:-1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "            # output = output[1:].view(-1, output.shape[-1])\n",
    "            # tgt = tgt[1:].reshape(-1) #tgt[:,:-1][1:].reshape(-1)\n",
    "            loss = loss_fn(output, tgt)\n",
    "            loss.backward()\n",
    "            clip = 1\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            losses += loss.item()\n",
    "            count+=1\n",
    "        except:\n",
    "            print(\"error found at \",i)\n",
    "                        # print(src.shape)\n",
    "#             print(tgt.shape)\n",
    "#             print(tgt[:,:-1].shape)\n",
    "#             print(i)\n",
    "\n",
    "#             print(batch.SRC)\n",
    "#             print(batch.TRG)\n",
    "#             print(\"i=\",i)\n",
    "#             lst=batch.SRC.tolist()\n",
    "#             print(lst)\n",
    "#             for b in lst:\n",
    "#                 print(\"length of file\", len(b))\n",
    "#                 for j in b:\n",
    "#                     print(SRC.vocab.itos[j])\n",
    "#                 print(\"------\")\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    return losses / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "#     val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "#     val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    count=0\n",
    "    for i,batch in enumerate(train_iterator):\n",
    "        src = batch.SRC.to(device)\n",
    "        tgt = batch.TRG.to(device)[:,:-1]\n",
    "        \n",
    "        output, _ = model(src, tgt[:,:-1])\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "        # output = output[1:].view(-1, output.shape[-1])\n",
    "        # tgt = tgt[1:].reshape(-1)\n",
    "        loss = loss_fn(output, tgt)\n",
    "        losses += loss.item()\n",
    "        count+=1\n",
    "    return losses / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.765,Val loss: 0.530 Train PPL:   2.148  Epoch time = 7.023s\n",
      "Epoch: 2, Train loss: 0.317,Val loss: 0.261 Train PPL:   1.373  Epoch time = 7.863s\n",
      "Epoch: 3, Train loss: 0.203,Val loss: 0.194 Train PPL:   1.225  Epoch time = 7.125s\n",
      "Epoch: 4, Train loss: 0.136,Val loss: 0.099 Train PPL:   1.146  Epoch time = 6.958s\n",
      "Epoch: 5, Train loss: 0.092,Val loss: 0.057 Train PPL:   1.096  Epoch time = 7.878s\n",
      "Epoch: 6, Train loss: 0.069,Val loss: 0.072 Train PPL:   1.071  Epoch time = 7.286s\n",
      "Epoch: 7, Train loss: 0.052,Val loss: 0.038 Train PPL:   1.053  Epoch time = 7.375s\n",
      "Epoch: 8, Train loss: 0.048,Val loss: 0.047 Train PPL:   1.050  Epoch time = 7.911s\n",
      "Epoch: 9, Train loss: 0.038,Val loss: 0.021 Train PPL:   1.038  Epoch time = 7.148s\n",
      "Epoch: 10, Train loss: 0.039,Val loss: 0.019 Train PPL:   1.040  Epoch time = 7.627s\n",
      "Epoch: 11, Train loss: 0.026,Val loss: 0.034 Train PPL:   1.026  Epoch time = 7.885s\n",
      "Epoch: 12, Train loss: 0.028,Val loss: 0.015 Train PPL:   1.029  Epoch time = 6.923s\n",
      "Epoch: 13, Train loss: 0.026,Val loss: 0.027 Train PPL:   1.027  Epoch time = 7.695s\n",
      "Epoch: 14, Train loss: 0.028,Val loss: 0.024 Train PPL:   1.029  Epoch time = 7.571s\n",
      "Epoch: 15, Train loss: 0.023,Val loss: 0.032 Train PPL:   1.023  Epoch time = 7.062s\n",
      "Epoch: 16, Train loss: 0.026,Val loss: 0.023 Train PPL:   1.026  Epoch time = 7.808s\n",
      "Epoch: 17, Train loss: 0.019,Val loss: 0.009 Train PPL:   1.019  Epoch time = 7.356s\n",
      "Epoch: 18, Train loss: 0.019,Val loss: 0.023 Train PPL:   1.019  Epoch time = 6.977s\n",
      "Epoch: 19, Train loss: 0.023,Val loss: 0.011 Train PPL:   1.023  Epoch time = 7.817s\n",
      "Epoch: 20, Train loss: 0.019,Val loss: 0.026 Train PPL:   1.019  Epoch time = 7.124s\n",
      "Epoch: 21, Train loss: 0.019,Val loss: 0.030 Train PPL:   1.020  Epoch time = 7.270s\n",
      "Epoch: 22, Train loss: 0.021,Val loss: 0.016 Train PPL:   1.021  Epoch time = 7.858s\n",
      "Epoch: 23, Train loss: 0.016,Val loss: 0.012 Train PPL:   1.016  Epoch time = 7.058s\n",
      "Epoch: 24, Train loss: 0.013,Val loss: 0.008 Train PPL:   1.013  Epoch time = 7.687s\n",
      "Epoch: 25, Train loss: 0.009,Val loss: 0.009 Train PPL:   1.009  Epoch time = 7.912s\n",
      "Epoch: 26, Train loss: 0.013,Val loss: 0.005 Train PPL:   1.013  Epoch time = 6.948s\n",
      "Epoch: 27, Train loss: 0.011,Val loss: 0.006 Train PPL:   1.011  Epoch time = 7.661s\n",
      "Epoch: 28, Train loss: 0.013,Val loss: 0.005 Train PPL:   1.013  Epoch time = 7.545s\n",
      "Epoch: 29, Train loss: 0.013,Val loss: 0.008 Train PPL:   1.013  Epoch time = 6.959s\n",
      "Epoch: 30, Train loss: 0.014,Val loss: 0.007 Train PPL:   1.014  Epoch time = 7.867s\n",
      "Epoch: 31, Train loss: 0.018,Val loss: 0.007 Train PPL:   1.019  Epoch time = 7.240s\n",
      "Epoch: 32, Train loss: 0.012,Val loss: 0.014 Train PPL:   1.012  Epoch time = 7.010s\n",
      "Epoch: 33, Train loss: 0.014,Val loss: 0.007 Train PPL:   1.014  Epoch time = 7.881s\n",
      "Epoch: 34, Train loss: 0.014,Val loss: 0.016 Train PPL:   1.014  Epoch time = 7.190s\n",
      "Epoch: 35, Train loss: 0.028,Val loss: 0.038 Train PPL:   1.029  Epoch time = 7.226s\n",
      "Epoch: 36, Train loss: 0.019,Val loss: 0.008 Train PPL:   1.019  Epoch time = 7.859s\n",
      "Epoch: 37, Train loss: 0.017,Val loss: 0.010 Train PPL:   1.017  Epoch time = 7.139s\n",
      "Epoch: 38, Train loss: 0.117,Val loss: 0.034 Train PPL:   1.124  Epoch time = 7.646s\n",
      "Epoch: 39, Train loss: 0.055,Val loss: 0.013 Train PPL:   1.056  Epoch time = 8.388s\n",
      "Epoch: 40, Train loss: 0.017,Val loss: 0.009 Train PPL:   1.017  Epoch time = 7.966s\n",
      "Epoch: 41, Train loss: 0.017,Val loss: 0.030 Train PPL:   1.017  Epoch time = 8.118s\n",
      "Epoch: 42, Train loss: 0.015,Val loss: 0.006 Train PPL:   1.015  Epoch time = 8.524s\n",
      "Epoch: 43, Train loss: 0.016,Val loss: 0.010 Train PPL:   1.016  Epoch time = 8.136s\n",
      "Epoch: 44, Train loss: 0.012,Val loss: 0.004 Train PPL:   1.012  Epoch time = 7.856s\n",
      "Epoch: 45, Train loss: 0.008,Val loss: 0.009 Train PPL:   1.008  Epoch time = 8.490s\n",
      "Epoch: 46, Train loss: 0.013,Val loss: 0.014 Train PPL:   1.013  Epoch time = 8.720s\n",
      "Epoch: 47, Train loss: 0.018,Val loss: 0.011 Train PPL:   1.018  Epoch time = 8.181s\n",
      "Epoch: 48, Train loss: 0.016,Val loss: 0.010 Train PPL:   1.016  Epoch time = 8.470s\n",
      "Epoch: 49, Train loss: 0.020,Val loss: 0.004 Train PPL:   1.020  Epoch time = 8.692s\n",
      "Epoch: 50, Train loss: 0.010,Val loss: 0.006 Train PPL:   1.010  Epoch time = 8.162s\n",
      "Epoch: 51, Train loss: 0.017,Val loss: 0.019 Train PPL:   1.017  Epoch time = 7.930s\n",
      "Epoch: 52, Train loss: 0.019,Val loss: 0.008 Train PPL:   1.019  Epoch time = 8.290s\n",
      "Epoch: 53, Train loss: 0.009,Val loss: 0.006 Train PPL:   1.009  Epoch time = 8.379s\n",
      "Epoch: 54, Train loss: 0.015,Val loss: 0.006 Train PPL:   1.015  Epoch time = 7.640s\n",
      "Epoch: 55, Train loss: 0.016,Val loss: 0.016 Train PPL:   1.017  Epoch time = 8.274s\n",
      "Epoch: 56, Train loss: 0.018,Val loss: 0.019 Train PPL:   1.018  Epoch time = 8.462s\n",
      "Epoch: 57, Train loss: 0.016,Val loss: 0.005 Train PPL:   1.017  Epoch time = 7.619s\n",
      "Epoch: 58, Train loss: 0.012,Val loss: 0.011 Train PPL:   1.012  Epoch time = 8.199s\n",
      "Epoch: 59, Train loss: 0.019,Val loss: 0.012 Train PPL:   1.019  Epoch time = 8.639s\n",
      "Epoch: 60, Train loss: 0.011,Val loss: 0.011 Train PPL:   1.011  Epoch time = 8.373s\n",
      "Epoch: 61, Train loss: 0.015,Val loss: 0.003 Train PPL:   1.015  Epoch time = 8.252s\n",
      "Epoch: 62, Train loss: 0.011,Val loss: 0.002 Train PPL:   1.011  Epoch time = 8.944s\n",
      "Epoch: 63, Train loss: 0.005,Val loss: 0.005 Train PPL:   1.005  Epoch time = 9.003s\n",
      "Epoch: 64, Train loss: 0.016,Val loss: 0.008 Train PPL:   1.016  Epoch time = 8.433s\n",
      "Epoch: 65, Train loss: 0.015,Val loss: 0.007 Train PPL:   1.015  Epoch time = 8.228s\n",
      "Epoch: 66, Train loss: 0.007,Val loss: 0.007 Train PPL:   1.007  Epoch time = 8.985s\n",
      "Epoch: 67, Train loss: 0.012,Val loss: 0.011 Train PPL:   1.012  Epoch time = 8.964s\n",
      "Epoch: 68, Train loss: 0.012,Val loss: 0.006 Train PPL:   1.012  Epoch time = 8.335s\n",
      "Epoch: 69, Train loss: 0.013,Val loss: 0.007 Train PPL:   1.013  Epoch time = 8.506s\n",
      "Epoch: 70, Train loss: 0.008,Val loss: 0.006 Train PPL:   1.008  Epoch time = 8.844s\n",
      "Epoch: 71, Train loss: 0.017,Val loss: 0.013 Train PPL:   1.018  Epoch time = 8.766s\n",
      "Epoch: 72, Train loss: 0.018,Val loss: 0.013 Train PPL:   1.018  Epoch time = 8.160s\n",
      "Epoch: 73, Train loss: 0.019,Val loss: 0.010 Train PPL:   1.019  Epoch time = 8.724s\n",
      "Epoch: 74, Train loss: 0.017,Val loss: 0.014 Train PPL:   1.017  Epoch time = 9.035s\n",
      "Epoch: 75, Train loss: 0.012,Val loss: 0.005 Train PPL:   1.012  Epoch time = 8.511s\n",
      "Epoch: 76, Train loss: 0.011,Val loss: 0.003 Train PPL:   1.011  Epoch time = 8.205s\n",
      "Epoch: 77, Train loss: 0.007,Val loss: 0.002 Train PPL:   1.007  Epoch time = 8.863s\n",
      "Epoch: 78, Train loss: 0.008,Val loss: 0.009 Train PPL:   1.008  Epoch time = 8.998s\n",
      "Epoch: 79, Train loss: 0.010,Val loss: 0.011 Train PPL:   1.010  Epoch time = 8.421s\n",
      "Epoch: 80, Train loss: 0.023,Val loss: 0.007 Train PPL:   1.023  Epoch time = 8.365s\n",
      "Epoch: 81, Train loss: 0.010,Val loss: 0.010 Train PPL:   1.010  Epoch time = 8.986s\n",
      "Epoch: 82, Train loss: 0.016,Val loss: 0.004 Train PPL:   1.016  Epoch time = 9.074s\n",
      "Epoch: 83, Train loss: 0.018,Val loss: 0.017 Train PPL:   1.018  Epoch time = 8.414s\n",
      "Epoch: 84, Train loss: 0.030,Val loss: 0.006 Train PPL:   1.031  Epoch time = 8.623s\n",
      "Epoch: 85, Train loss: 0.050,Val loss: 0.015 Train PPL:   1.051  Epoch time = 8.932s\n",
      "Epoch: 86, Train loss: 0.013,Val loss: 0.013 Train PPL:   1.013  Epoch time = 9.060s\n",
      "Epoch: 87, Train loss: 0.013,Val loss: 0.007 Train PPL:   1.013  Epoch time = 8.476s\n",
      "Epoch: 88, Train loss: 0.009,Val loss: 0.005 Train PPL:   1.010  Epoch time = 8.817s\n",
      "Epoch: 89, Train loss: 0.006,Val loss: 0.003 Train PPL:   1.006  Epoch time = 9.121s\n",
      "Epoch: 90, Train loss: 0.006,Val loss: 0.003 Train PPL:   1.006  Epoch time = 9.130s\n",
      "Epoch: 91, Train loss: 0.006,Val loss: 0.004 Train PPL:   1.006  Epoch time = 8.401s\n",
      "Epoch: 92, Train loss: 0.011,Val loss: 0.003 Train PPL:   1.011  Epoch time = 8.755s\n",
      "Epoch: 93, Train loss: 0.008,Val loss: 0.003 Train PPL:   1.008  Epoch time = 9.141s\n",
      "Epoch: 94, Train loss: 0.012,Val loss: 0.009 Train PPL:   1.012  Epoch time = 9.076s\n",
      "Epoch: 95, Train loss: 0.012,Val loss: 0.004 Train PPL:   1.012  Epoch time = 8.537s\n",
      "Epoch: 96, Train loss: 0.005,Val loss: 0.015 Train PPL:   1.005  Epoch time = 8.891s\n",
      "Epoch: 97, Train loss: 0.013,Val loss: 0.009 Train PPL:   1.013  Epoch time = 9.046s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98, Train loss: 0.015,Val loss: 0.010 Train PPL:   1.015  Epoch time = 9.010s\n",
      "Epoch: 99, Train loss: 0.021,Val loss: 0.007 Train PPL:   1.021  Epoch time = 8.384s\n",
      "Epoch: 100, Train loss: 0.015,Val loss: 0.008 Train PPL:   1.015  Epoch time = 8.919s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f},Val loss: {val_loss:.3f} Train PPL: {math.exp(train_loss):7.3f}  \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.019490875619552792,\n",
       " 0.020752492833301578,\n",
       " 0.015918305809077053,\n",
       " 0.013010261928394221,\n",
       " 0.008659975513279626,\n",
       " 0.013147805170484254,\n",
       " 0.010607275094948722,\n",
       " 0.012777534914927488,\n",
       " 0.01267196880925962,\n",
       " 0.01359922630589163,\n",
       " 0.018356752405104107,\n",
       " 0.011778397275539898,\n",
       " 0.014258318769552329,\n",
       " 0.01416218710917437,\n",
       " 0.028394433888334267,\n",
       " 0.019155865299522856,\n",
       " 0.016868898217000335,\n",
       " 0.1166336673585589,\n",
       " 0.05484068961231969,\n",
       " 0.017231686754422163,\n",
       " 0.01718949526532905,\n",
       " 0.014749374000530224,\n",
       " 0.016068015724390823,\n",
       " 0.012019557247147029,\n",
       " 0.00817884084136359,\n",
       " 0.013355585250811852,\n",
       " 0.017860552735386438,\n",
       " 0.015933393931033046,\n",
       " 0.020067394310744738,\n",
       " 0.009666671733089544,\n",
       " 0.01734452664405058,\n",
       " 0.019286131571595647,\n",
       " 0.008508756438962511,\n",
       " 0.01531101446287605,\n",
       " 0.01636688795607621,\n",
       " 0.017904658697203712,\n",
       " 0.016383062876984503,\n",
       " 0.01194023420217452,\n",
       " 0.018909380248202223,\n",
       " 0.011150494424482725,\n",
       " 0.014875521887122576,\n",
       " 0.010550435761576816,\n",
       " 0.004598363881241505,\n",
       " 0.016354323129409286,\n",
       " 0.014526334380748945,\n",
       " 0.007419075503152116,\n",
       " 0.011825471340187582,\n",
       " 0.012153619724639166,\n",
       " 0.012696726569513286,\n",
       " 0.008309652131077762,\n",
       " 0.01749449706960846,\n",
       " 0.01797092928237807,\n",
       " 0.01860312006744046,\n",
       " 0.01666282080383253,\n",
       " 0.01221576404938613,\n",
       " 0.010688472961966058,\n",
       " 0.007193584207751438,\n",
       " 0.008459732326492544,\n",
       " 0.009933323654354837,\n",
       " 0.022818601146587132,\n",
       " 0.010269853650765498,\n",
       " 0.015773066895332144,\n",
       " 0.017530858685975235,\n",
       " 0.030294882855625784,\n",
       " 0.05007749507847698,\n",
       " 0.013156663603830507,\n",
       " 0.012860703479979767,\n",
       " 0.009467543922042711,\n",
       " 0.006246653680678487,\n",
       " 0.005648440450657174,\n",
       " 0.005899445967906046,\n",
       " 0.01067209452796615,\n",
       " 0.007921952863204964,\n",
       " 0.011996241109451883,\n",
       " 0.01207635394391044,\n",
       " 0.005142740064834373,\n",
       " 0.012788073746784728,\n",
       " 0.015355168139726726,\n",
       " 0.02093611945716436,\n",
       " 0.014631715967435083,\n",
       " 0.015563870087483318,\n",
       " 0.02653021289579362,\n",
       " 0.016035851552163527,\n",
       " 0.015182886093711243,\n",
       " 0.02546636976741014,\n",
       " 0.026335898183033543,\n",
       " 0.017084034723124437,\n",
       " 0.011117771959571327,\n",
       " 0.011132629290241699,\n",
       " 0.015282580755933367,\n",
       " 0.015721789827094133,\n",
       " 0.019090756341725825,\n",
       " 0.01508646693586575,\n",
       " 0.009880863559064505,\n",
       " 0.010218627475393425,\n",
       " 0.013422792537776913,\n",
       " 0.013561138900184504,\n",
       " 0.008430040660080502,\n",
       " 0.01186971633262843,\n",
       " 0.007391209109203934]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.764539</td>\n",
       "      <td>0.530184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.316991</td>\n",
       "      <td>0.260702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.203184</td>\n",
       "      <td>0.194429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136467</td>\n",
       "      <td>0.098934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.091537</td>\n",
       "      <td>0.057138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.014807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.012788</td>\n",
       "      <td>0.009267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.015355</td>\n",
       "      <td>0.010281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.020936</td>\n",
       "      <td>0.007396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.014632</td>\n",
       "      <td>0.008198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  val_loss\n",
       "0     0.764539  0.530184\n",
       "1     0.316991  0.260702\n",
       "2     0.203184  0.194429\n",
       "3     0.136467  0.098934\n",
       "4     0.091537  0.057138\n",
       "..         ...       ...\n",
       "95    0.005143  0.014807\n",
       "96    0.012788  0.009267\n",
       "97    0.015355  0.010281\n",
       "98    0.020936  0.007396\n",
       "99    0.014632  0.008198\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats=pd.DataFrame(list(train_losses))\n",
    "df_stats['val_losses']=list(val_losses)\n",
    "df_stats.columns=['train_loss','val_loss']\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from  matplotlib.ticker import FuncFormatter\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['train_loss'], 'b-o', label=\"Train Loss\")\n",
    "plt.plot(df_stats['val_loss'], 'g-o', label=\"Validation Loss\")\n",
    "#plt.plot(df_stats['PPL'], 'r-o', label=\"Perplexity\")\n",
    "#plt.plot(df_stats['BERTSCORE'], 'c-o', label=\"Bert Score\")\n",
    "#plt.plot(df_stats[plotTypeVar[i]], colorArr[i], label=labelsArr[i])\n",
    "# Label the plot.\n",
    "plt.title(\"Training Vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.legend()\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: (int(x))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch: 1, Train loss: 4.027, Train PPL:  56.102 | Val loss: 3.030, Val. PPL:  20.690 | Epoch time = 21.458s\n",
    "Epoch: 2, Train loss: 2.643, Train PPL:  14.049 | Val loss: 2.407, Val. PPL:  11.095 | Epoch time = 21.550s\n",
    "Epoch: 3, Train loss: 2.072, Train PPL:   7.944 | Val loss: 2.182, Val. PPL:   8.864 | Epoch time = 21.634s\n",
    "Epoch: 4, Train loss: 1.703, Train PPL:   5.492 | Val loss: 2.080, Val. PPL:   8.002 | Epoch time = 21.682s\n",
    "Epoch: 5, Train loss: 1.435, Train PPL:   4.199 | Val loss: 2.045, Val. PPL:   7.730 | Epoch time = 21.998s\n",
    "Epoch: 6, Train loss: 1.225, Train PPL:   3.403 | Val loss: 2.045, Val. PPL:   7.727 | Epoch time = 22.008s\n",
    "Epoch: 7, Train loss: 1.049, Train PPL:   2.854 | Val loss: 2.051, Val. PPL:   7.777 | Epoch time = 22.021s\n",
    "Epoch: 8, Train loss: 0.905, Train PPL:   2.472 | Val loss: 2.121, Val. PPL:   8.342 | Epoch time = 21.982s\n",
    "Epoch: 9, Train loss: 0.792, Train PPL:   2.209 | Val loss: 2.179, Val. PPL:   8.834 | Epoch time = 22.050s\n",
    "Epoch: 10, Train loss: 0.700, Train PPL:   2.014 | Val loss: 2.217, Val. PPL:   9.178 | Epoch time = 22.096s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "#     if isinstance(sentence, str):\n",
    "#         src_tensor = text_transform[SRC_LANGUAGE](sentence).unsqueeze(0).to(device)\n",
    "#     else:\n",
    "#         src_tensor = text_transform[TGT_LANGUAGE](sentence).unsqueeze(0).to(device)\n",
    "    \n",
    "#     src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    #trg_indexes = [trg_field[init_token]]\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        #if pred_token == trg_field[eos_token]:\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    #trg_tokens = [trg_field.vocab.get_itos()[i] for i in trg_indexes]\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src =  can we create a constant for week month then iterate it ?\n",
      "trg = buggycode suggestion\n",
      "predicted trg = refactoring suggestion <eos>\n",
      "****************************************\n",
      "src =  please get it verified and update the pr once verified .\n",
      "trg = completeness suggestion\n",
      "predicted trg = completeness suggestion <eos>\n",
      "****************************************\n",
      "src =  its a default part of the gst gst cgst sgst igst cess advalorem state cess\n",
      "trg = logical suggestion\n",
      "predicted trg = logistics suggestion <eos>\n",
      "****************************************\n",
      "src =  please use isrequired feature of the formdefinitions .\n",
      "trg = reuse suggestion\n",
      "predicted trg = logical suggestion <eos>\n",
      "****************************************\n",
      "src =    atomicreference any specific region to use this here ?\n",
      "trg = code suggestion\n",
      "predicted trg = questioning <eos>\n",
      "****************************************\n",
      "src =  can we extract this to a common place ? is there a chance for any other modules to use it ?\n",
      "trg = refactoring suggestion\n",
      "predicted trg = code suggestion <eos>\n",
      "****************************************\n",
      "src =  you can do a stronger check with stringutils .isempty but upto you\n",
      "trg = logical suggestion\n",
      "predicted trg = logical suggestion <eos>\n",
      "****************************************\n",
      "src =  please verify and update pr .\n",
      "trg = completeness suggestion\n",
      "predicted trg = completeness suggestion <eos>\n",
      "****************************************\n",
      "src =  did you test this change e a b fe a c ad c\n",
      "trg = completeness suggestion\n",
      "predicted trg = completeness suggestion <eos>\n",
      "****************************************\n",
      "src =  please merge after verification .\n",
      "trg = completeness suggestion\n",
      "predicted trg = completeness suggestion <eos>\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "for example_idx in range(20,30):\n",
    "\n",
    "    src = vars(test_data.examples[example_idx])['SRC']\n",
    "    trg = vars(test_data.examples[example_idx])['TRG']\n",
    "\n",
    "    print(\"src = \",' '.join(src))\n",
    "    print(\"trg =\" ,' '.join(trg))\n",
    "\n",
    "    translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "    print(\"predicted trg =\" ,' '.join(translation))\n",
    "    print(\"*\"*40)\n",
    "    #print(f'predicted trg = {' '.join(translation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
    "    \n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                           rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = buggycode suggestion <eos>\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(\"nope we will harcode color like this\", SRC, TRG, model, device)\n",
    "print(\"predicted trg =\" ,' '.join(translation))\n",
    "print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
